# Reseraching

## Methodology

1. Commencer par rechercher le processus de création. Par exemple, pour récupérer du texte dans une image, on peut regarder comment cacher du texte dans une image.
2. Choisir les liens qui collent le plus et lire le contenu (favoriser les sites avec la meiller reputation comme null-bytes en secu)
3. Choisir les termes qui collent le mieux à la recherche à partir de la premiere recherche (Ex : terme Steganography ressort quand on cherche pur cacher du texte)
4. Chercher ce que l'on veut faire à partir des termes précis ex : "Extract file from jpeg Steganography"
5. google les termes inconnus pour bien comprendre et avancer (moins on connait, plus c'est redondant mais bon)

## Vulnerability Searching

Il y a des sites de regroupement des vulnérabilités :

* ExploitDB - Exploit qu'on peut directement télécharger et prêt à use
* NVD (Tracks of CVE => Common Vulnerability and Exposures, avec pour le format CVE-YEAR-IDNumber
* CVE Mitre

NVD et ExploitDB marche ensemble car avec le CVE de l'exploit DB, on peut récupérer toutes les informations sur ce sujet pour pouvoir sep rotéger.

## Search Engine (https://tryhackme.com/room/googledorking)

### Crawler

Google utilise des Crawler/Spiders pour parcourcir internet et indexer (Repertorier) son contenu en les liant à des mots clés. Les crawlers parcourt le contenu de plusieurs façons :

* En visitant les URL et les informations de son contenu sont renvoyés vers le moteur de recherche
* En suivant les URLS dans le contenu trouvés dans des URL visités comme un virus, il va s'étendre sur toutes les URLS trouvés pour les scanner

#### Fonctionnement
image::https://i.imgur.com/4nrDDa0.png[crawler_schema]

Le crawler va sur le site, récupere le contenu et en fait un dictionnary avec des mots clés/images et les référence dane le search engine. Si le site contient une URL vers un site, il ira sur ce site et effectuera le même processus et ainsi de suite pour alimenter le dictionnaire et lier des sites avec des mots clés.

Pour choisir la hiérarchie, on se base sur certains critères (SEO) qui donne un Score comme :

* Responsiveness
* facilité du Crawling (avec des sitemaps, Robots.txt qui permet de limiter les pages indexés et mieux controller le réferencement etc)
* Le type de Keywords et le nombre de site compétitif
* Service d'avertisement

Des sites renseignent sur ces informations comme : https://web.dev/

#### Robots.txt
Cela permet de limiter le type de Crawl qui accède au sites (Google, MSN, Firefox, etc) et les fichiers (Pages/directories) qui sont autorisés ou non à crawl (souvent les éléments qui ne sont pas autorisés sont des bons points d'entrées à pentest) :

* User-agent : Type de Crawler
* Allow : Page autorisée (Non obligatoire)
* Disallow : Page interdite pour le crawler (Non obligatoire, si y a que une page, toutes les autres seront donc autorisées)
* Sitemap (Provide a reference to where the sitemap is located)

Exemple: 

image::https://i.imgur.com/audlFn8.png[robots.txt]

Si on veut qu'un seul crawler on peut faire quelqe chose comme user-agent : Googlebot

On peut aussi utiliser des regex pour faciliter comme tous les ficheirs en .ini 

image::https://i.imgur.com/mzDqFVY.png[regex_robot.txt]

#### Sitemaps
C'est la carte d'un site qui permet d'aider les crawlers à trouver les routes pour le coontenu et cela correspond à la structure d'un site ex (visuel):

image::https://i.imgur.com/L5WqJU4.png[sitemap]

Blue rectangle => Nested contents
Green Rectangle => Pages

Un Sitemap a une structure en xml donc c'est un format xml (pas un graph) et cela permet de faciliter le job car il n'a pas besoin de le faire donc ça accelere son travail et l'évite de tout faire manuelelment (Comparaison à utiliser une liste de mots à trouve dans un texte au lieu de taper des mots au pif pour voir s'ils sont dans le texte)


### Dorking (Advanced Searching)

Google a plein d'élements pour faciliter les recherches comme :

* opérations mathématiques (directement dans la barre de recherche)
* Quotation mark pour limiter à tout ce qui y a EXACTEMEENT entre les guillements : "seulement si y a chacun de ses mots"
* site, on peut utiliser le nom d'un site avant une recherche pour avoir les résulat que de ce site : site:bbc.co.uk qchq news

Liste des termes  (cumuables ex :site:bbc.co.uk filetype:pdf") :

* site:{name} {serach} pour chercher sur un site uniquement
* filetype:{type} {serach} pour récupérer uniqument des fichiers de ce type
* cache{url} version cache de google d'une URL specifique
* intitle:{phrase} cette phrase doit etre dans le titre de la page
* inurl:{phrase} pareil que intitle mais pour l'URL
* intext:{words} pareil mais pour le texte
* Asterisk(*) permet de mettre un blank exemple : How to * a website (How to design/create/reference/hack/publish/Host/...)
* Negation(-) permet d'enlever des termes exemples débrilateur -wow pour avoir les obejts de wow
* Concatenate (+) permet de combiner des termes
* OR (|) pour avoir un mot Ou l'autre

liste des googles dork : https://gbhackers.com/latest-google-dorks-list/
