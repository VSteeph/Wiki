# Reseraching

## Methodology

1. Commencer par rechercher le processus de création. Par exemple, pour récupérer du texte dans une image, on peut regarder comment cacher du texte dans une image.
2. Choisir les liens qui collent le plus et lire le contenu (favoriser les sites avec la meiller reputation comme null-bytes en secu)
3. Choisir les termes qui collent le mieux à la recherche à partir de la premiere recherche (Ex : terme Steganography ressort quand on cherche pur cacher du texte)
4. Chercher ce que l'on veut faire à partir des termes précis ex : "Extract file from jpeg Steganography"
5. google les termes inconnus pour bien comprendre et avancer (moins on connait, plus c'est redondant mais bon)

## Vulnerability Searching

Il y a des sites de regroupement des vulnérabilités :
* ExploitDB - Exploit qu'on peut directement télécharger et prêt à use
* NVD (Tracks of CVE => Common Vulnerability and Exposures, avec pour le format CVE-YEAR-IDNumber
* CVE Mitre

NVD et ExploitDB marche ensemble car avec le CVE de l'exploit DB, on peut récupérer toutes les informations sur ce sujet pour pouvoir sep rotéger.

## Dorking (https://tryhackme.com/room/googledorking)

### Crawler

Google utilise des Crawler/Spiders pour parcourcir internet et indexer (Repertorier) son contenu en les liant à des mots clés. Les crawlers parcourt le contenu de plusieurs façons :
* En visitant les URL et les informations de son contenu sont renvoyés vers le moteur de recherche
* En suivant les URLS dans le contenu trouvés dans des URL visités comme un virus, il va s'étendre sur toutes les URLS trouvés pour les scanner

#### Fonctionnement
image::https://i.imgur.com/4nrDDa0.png[crawler_schema]

Le crawler va sur le site, récupere le contenu et en fait un dictionnary avec des mots clés/images et les référence dane le search engine. Si le site contient une URL vers un site, il ira sur ce site et effectuera le même processus et ainsi de suite pour alimenter le dictionnaire et lier des sites avec des mots clés.

Pour choisir la hiérarchie, on se base sur certains critères (SEO) qui donne un Score comme :
* Responsiveness
* facilité du Crawling (avec des sitemaps, Robots.txt qui permet de limiter les pages indexés et mieux controller le réferencement etc)
* Le type de Keywords et le nombre de site compétitif
* Service d'avertisement

Des sites renseignent sur ces informations comme : https://web.dev/

#### Robots.txt
Cela permet de limiter le type de Crawl qui accède au sites (Google, MSN, Firefox, etc) et les fichiers (Pages/directories) qui sont autorisés ou non à crawl (souvent les éléments qui ne sont pas autorisés sont des bons points d'entrées à pentest) :
* User-agent : Type de Crawler
* Allow : Page autorisée (Non obligatoire)
* Disallow : Page interdite pour le crawler (Non obligatoire, si y a que une page, toutes les autres seront donc autorisées)
* Sitemap (Provide a reference to where the sitemap is located)

Exemple: 

image::https://i.imgur.com/audlFn8.png[robots.txt]

Si on veut qu'un seul crawler on peut faire quelqe chose comme user-agent : Googlebot

On peut aussi utiliser des regex pour faciliter comme tous les ficheirs en .ini 

image::https://i.imgur.com/mzDqFVY.png[regex_robot.txt]

#### Sitemaps
C'est la carte d'un site qui permet d'aider les crawlers à trouver les routes pour le coontenu et cela correspond à la structure d'un site ex (visuel):

image::https://i.imgur.com/L5WqJU4.png[sitemap]

Blue rectangle => Nested contents
Green Rectangle => Pages

Un Sitemap a une structure en xml donc c'est un format xml (pas un graph) et cela permet de faciliter le job car il n'a pas besoin de le faire donc ça accelere son travail et l'évite de tout faire manuelelment (Comparaison à utiliser une liste de mots à trouve dans un texte au lieu de taper des mots au pif pour voir s'ils sont dans le texte)
